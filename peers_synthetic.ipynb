{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOlRp4KlrqV2BgyMbU4WKeo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/francescovenco000004/peersv02/blob/main/peers_synthetic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nUPakRQTU1wC",
        "outputId": "72b5906d-a222-4c36-860c-e0a9d450ffa8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.42.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.5)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.7.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### iteration (training data)"
      ],
      "metadata": {
        "id": "QjfRiyhg_Y0C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import csv\n",
        "from transformers import pipeline\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import os\n",
        "\n",
        "# Define the QA model and tokenizer\n",
        "model_name = \"deepset/roberta-base-squad2\"\n",
        "qa_pipeline = pipeline('question-answering', model=model_name, tokenizer=model_name)\n",
        "\n",
        "# Define the sentence transformer model\n",
        "sbert_model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
        "\n",
        "# Change directory to the 'peers_synthetic' folder at the root of Google Drive\n",
        "os.chdir('/content/drive/My Drive/peers_synthetic')\n",
        "\n",
        "# Function to read CSV files and return the first column as a list\n",
        "def read_csv(file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "    return df.iloc[:, 0].tolist()\n",
        "\n",
        "# Read names and descriptions from CSV files\n",
        "zava_names = read_csv('zava_names.csv')\n",
        "zava_descriptions = read_csv('zava_description.csv')\n",
        "borzo_names = read_csv('borzo_names.csv')\n",
        "borzo_descriptions = read_csv('borzo_description.csv')\n",
        "\n",
        "# Create dictionaries for bases and peers\n",
        "bases = {\n",
        "    \"zava\": zava_descriptions[0],\n",
        "    \"bozo\": borzo_descriptions[0]\n",
        "}\n",
        "\n",
        "peers = {\n",
        "    \"zava\": {zava_names[i]: zava_descriptions[i] for i in range(1, len(zava_names))},\n",
        "    \"bozo\": {borzo_names[i]: borzo_descriptions[i] for i in range(1, len(borzo_names))}\n",
        "}\n",
        "\n",
        "# Questions to ask\n",
        "questions = [\n",
        "    \"What does {} provide?\",\n",
        "    \"What is {} vertical focus?\",\n",
        "    \"Who are {} consumers?\"\n",
        "]\n",
        "\n",
        "# Function to perform information extraction\n",
        "def information_extraction(description, peer_name):\n",
        "    extracted_info = {}\n",
        "    for idx, question_template in enumerate(questions):\n",
        "        question = question_template.format(peer_name)\n",
        "        QA_input = {\n",
        "            'question': question,\n",
        "            'context': description,\n",
        "        }\n",
        "        res = qa_pipeline(QA_input)\n",
        "        extracted_info[f\"question_{idx+1}\"] = res['answer']\n",
        "    return extracted_info\n",
        "\n",
        "# Extract information for all peers\n",
        "def extract_all_information(bases, peers):\n",
        "    extracted_data = {\"zava\": {}, \"bozo\": {}}\n",
        "\n",
        "    for base in bases.keys():\n",
        "        for peer, description in peers[base].items():\n",
        "            extracted_info = information_extraction(description, peer)\n",
        "            extracted_data[base][peer] = extracted_info\n",
        "\n",
        "    return extracted_data\n",
        "\n",
        "# Calculate cosine similarity\n",
        "def calculate_cosine_similarity(extracted_data, bases):\n",
        "    similarities = []\n",
        "    for base_name, base_description in bases.items():\n",
        "        base_extracted = information_extraction(base_description, base_name)\n",
        "\n",
        "        for peer_name, peer_data in extracted_data[base_name].items():\n",
        "            for question in range(1, 4):\n",
        "                peer_info = peer_data[f\"question_{question}\"]\n",
        "                base_info = base_extracted[f\"question_{question}\"]\n",
        "\n",
        "                # Compute embeddings\n",
        "                peer_embedding = sbert_model.encode(peer_info, convert_to_tensor=True)\n",
        "                base_embedding = sbert_model.encode(base_info, convert_to_tensor=True)\n",
        "\n",
        "                # Compute cosine similarity\n",
        "                similarity = util.pytorch_cos_sim(peer_embedding, base_embedding).item()\n",
        "\n",
        "                similarities.append({\n",
        "                    \"peer\": peer_name,\n",
        "                    \"base\": base_name,\n",
        "                    \"question\": question,\n",
        "                    \"similarity\": similarity\n",
        "                })\n",
        "\n",
        "    return similarities\n",
        "\n",
        "# Extract information\n",
        "extracted_data = extract_all_information(bases, peers)\n",
        "\n",
        "# Compute cosine similarities\n",
        "similarities = calculate_cosine_similarity(extracted_data, bases)\n",
        "\n",
        "# Print results\n",
        "for sim in similarities:\n",
        "    print(f\"Peer: {sim['peer']}, Base: {sim['base']}, Question: {sim['question']}, Similarity: {sim['similarity']:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OeCtcEOU_ej6",
        "outputId": "a5b32871-687e-474f-c342-8b578d070fc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Peer: Healthily, Base: zava, Question: 1, Similarity: 0.4493\n",
            "Peer: Healthily, Base: zava, Question: 2, Similarity: 0.3115\n",
            "Peer: Healthily, Base: zava, Question: 3, Similarity: 0.4540\n",
            "Peer: Tele clinic, Base: zava, Question: 1, Similarity: 0.4335\n",
            "Peer: Tele clinic, Base: zava, Question: 2, Similarity: 0.4020\n",
            "Peer: Tele clinic, Base: zava, Question: 3, Similarity: 0.3246\n",
            "Peer: Practo, Base: zava, Question: 1, Similarity: 0.6025\n",
            "Peer: Practo, Base: zava, Question: 2, Similarity: 0.4912\n",
            "Peer: Practo, Base: zava, Question: 3, Similarity: 0.3350\n",
            "Peer: Urban, Base: zava, Question: 1, Similarity: 0.3421\n",
            "Peer: Urban, Base: zava, Question: 2, Similarity: 0.2763\n",
            "Peer: Urban, Base: zava, Question: 3, Similarity: 0.2691\n",
            "Peer: Intendu, Base: zava, Question: 1, Similarity: 0.1656\n",
            "Peer: Intendu, Base: zava, Question: 2, Similarity: 0.6781\n",
            "Peer: Intendu, Base: zava, Question: 3, Similarity: 0.2466\n",
            "Peer: Watch your health, Base: zava, Question: 1, Similarity: 0.7765\n",
            "Peer: Watch your health, Base: zava, Question: 2, Similarity: 0.4310\n",
            "Peer: Watch your health, Base: zava, Question: 3, Similarity: 0.2317\n",
            "Peer: Cura, Base: zava, Question: 1, Similarity: 0.6641\n",
            "Peer: Cura, Base: zava, Question: 2, Similarity: 0.4688\n",
            "Peer: Cura, Base: zava, Question: 3, Similarity: 0.4585\n",
            "Peer: Health hero, Base: zava, Question: 1, Similarity: 0.3800\n",
            "Peer: Health hero, Base: zava, Question: 2, Similarity: 0.7702\n",
            "Peer: Health hero, Base: zava, Question: 3, Similarity: 0.2499\n",
            "Peer: Biohm, Base: zava, Question: 1, Similarity: 0.3347\n",
            "Peer: Biohm, Base: zava, Question: 2, Similarity: 0.1594\n",
            "Peer: Biohm, Base: zava, Question: 3, Similarity: 0.2836\n",
            "Peer: Qured, Base: zava, Question: 1, Similarity: 0.5368\n",
            "Peer: Qured, Base: zava, Question: 2, Similarity: 0.4003\n",
            "Peer: Qured, Base: zava, Question: 3, Similarity: 0.3037\n",
            "Peer: Klikdoctor, Base: zava, Question: 1, Similarity: 0.3758\n",
            "Peer: Klikdoctor, Base: zava, Question: 2, Similarity: 0.3911\n",
            "Peer: Klikdoctor, Base: zava, Question: 3, Similarity: 0.2565\n",
            "Peer: Body collective, Base: zava, Question: 1, Similarity: 0.4021\n",
            "Peer: Body collective, Base: zava, Question: 2, Similarity: 0.3133\n",
            "Peer: Body collective, Base: zava, Question: 3, Similarity: 0.4424\n",
            "Peer: Penda health, Base: zava, Question: 1, Similarity: 0.4898\n",
            "Peer: Penda health, Base: zava, Question: 2, Similarity: 0.3704\n",
            "Peer: Penda health, Base: zava, Question: 3, Similarity: 0.3637\n",
            "Peer: Pathkind labs, Base: zava, Question: 1, Similarity: 0.3663\n",
            "Peer: Pathkind labs, Base: zava, Question: 2, Similarity: 0.3645\n",
            "Peer: Pathkind labs, Base: zava, Question: 3, Similarity: 0.4222\n",
            "Peer: Lora, Base: zava, Question: 1, Similarity: 0.5721\n",
            "Peer: Lora, Base: zava, Question: 2, Similarity: 0.5586\n",
            "Peer: Lora, Base: zava, Question: 3, Similarity: 0.3920\n",
            "Peer: Gene box, Base: zava, Question: 1, Similarity: 0.2995\n",
            "Peer: Gene box, Base: zava, Question: 2, Similarity: 0.3229\n",
            "Peer: Gene box, Base: zava, Question: 3, Similarity: 0.3461\n",
            "Peer: Bosta, Base: bozo, Question: 1, Similarity: 0.8770\n",
            "Peer: Bosta, Base: bozo, Question: 2, Similarity: 0.5413\n",
            "Peer: Bosta, Base: bozo, Question: 3, Similarity: 0.6168\n",
            "Peer: Zoomo, Base: bozo, Question: 1, Similarity: 0.3396\n",
            "Peer: Zoomo, Base: bozo, Question: 2, Similarity: 0.3426\n",
            "Peer: Zoomo, Base: bozo, Question: 3, Similarity: 0.4154\n",
            "Peer: Grovy, Base: bozo, Question: 1, Similarity: 0.4835\n",
            "Peer: Grovy, Base: bozo, Question: 2, Similarity: 0.5323\n",
            "Peer: Grovy, Base: bozo, Question: 3, Similarity: 0.5041\n",
            "Peer: Vahak, Base: bozo, Question: 1, Similarity: 0.5991\n",
            "Peer: Vahak, Base: bozo, Question: 2, Similarity: 0.3509\n",
            "Peer: Vahak, Base: bozo, Question: 3, Similarity: 0.3544\n",
            "Peer: Drive yello, Base: bozo, Question: 1, Similarity: 0.8006\n",
            "Peer: Drive yello, Base: bozo, Question: 2, Similarity: 0.5883\n",
            "Peer: Drive yello, Base: bozo, Question: 3, Similarity: 0.8052\n",
            "Peer: Paps, Base: bozo, Question: 1, Similarity: 0.9053\n",
            "Peer: Paps, Base: bozo, Question: 2, Similarity: 0.3689\n",
            "Peer: Paps, Base: bozo, Question: 3, Similarity: 0.4154\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import csv\n",
        "from transformers import pipeline\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import os\n",
        "\n",
        "# Define the QA model and tokenizer\n",
        "model_name = \"deepset/roberta-base-squad2\"\n",
        "qa_pipeline = pipeline('question-answering', model=model_name, tokenizer=model_name)\n",
        "\n",
        "# Define the sentence transformer model\n",
        "sbert_model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
        "\n",
        "# Change directory to the 'peers_synthetic' folder at the root of Google Drive\n",
        "os.chdir('/content/drive/My Drive/peers_synthetic')\n",
        "\n",
        "# Function to read CSV files and return the first column as a list\n",
        "def read_csv(file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "    return df.iloc[:, 0].tolist()\n",
        "\n",
        "# Read names and descriptions from CSV files\n",
        "zava_names = read_csv('csv/zava_names.csv')\n",
        "zava_descriptions = read_csv('csv/zava_description.csv')\n",
        "borzo_names = read_csv('csv/borzo_names.csv')\n",
        "borzo_descriptions = read_csv('csv/borzo_description.csv')\n",
        "\n",
        "# Create dictionaries for bases and peers\n",
        "bases = {\n",
        "    \"zava\": zava_descriptions[0],\n",
        "    \"bozo\": borzo_descriptions[0]\n",
        "}\n",
        "\n",
        "peers = {\n",
        "    \"zava\": {zava_names[i]: zava_descriptions[i] for i in range(1, len(zava_names))},\n",
        "    \"bozo\": {borzo_names[i]: borzo_descriptions[i] for i in range(1, len(borzo_names))}\n",
        "}\n",
        "\n",
        "# Questions to ask\n",
        "questions = [\n",
        "    \"What does {} provide?\",\n",
        "    \"What is {} vertical focus?\",\n",
        "    \"Who are {} consumers?\"\n",
        "]\n",
        "\n",
        "# Function to perform information extraction\n",
        "def information_extraction(description, peer_name):\n",
        "    extracted_info = {}\n",
        "    for idx, question_template in enumerate(questions):\n",
        "        question = question_template.format(peer_name)\n",
        "        QA_input = {\n",
        "            'question': question,\n",
        "            'context': description,\n",
        "        }\n",
        "        res = qa_pipeline(QA_input)\n",
        "        extracted_info[f\"question_{idx+1}\"] = res['answer']\n",
        "    return extracted_info\n",
        "\n",
        "# Extract information for all peers\n",
        "def extract_all_information(bases, peers):\n",
        "    extracted_data = {\"zava\": {}, \"bozo\": {}}\n",
        "\n",
        "    for base in bases.keys():\n",
        "        for peer, description in peers[base].items():\n",
        "            extracted_info = information_extraction(description, peer)\n",
        "            extracted_data[base][peer] = extracted_info\n",
        "\n",
        "    return extracted_data\n",
        "\n",
        "# Calculate transformer-based similarity\n",
        "def calculate_transformer_similarity(extracted_data, bases):\n",
        "    similarities = []\n",
        "    similarity_dict = {}\n",
        "\n",
        "    for base_name, base_description in bases.items():\n",
        "        base_extracted = information_extraction(base_description, base_name)\n",
        "        similarity_dict[base_name] = {}\n",
        "\n",
        "        for peer_name, peer_data in extracted_data[base_name].items():\n",
        "            similarity_dict[base_name][peer_name] = {}\n",
        "\n",
        "            for question in range(1, 3):\n",
        "                peer_info = peer_data[f\"question_{question}\"]\n",
        "                base_info = base_extracted[f\"question_{question}\"]\n",
        "\n",
        "                # Compute embeddings\n",
        "                peer_embedding = sbert_model.encode(peer_info, convert_to_tensor=True)\n",
        "                base_embedding = sbert_model.encode(base_info, convert_to_tensor=True)\n",
        "\n",
        "                # Compute similarity\n",
        "                similarity = util.pytorch_cos_sim(peer_embedding, base_embedding).item()\n",
        "\n",
        "                similarities.append({\n",
        "                    \"peer\": peer_name,\n",
        "                    \"base\": base_name,\n",
        "                    \"question\": question,\n",
        "                    \"similarity\": similarity\n",
        "                })\n",
        "\n",
        "                similarity_dict[base_name][peer_name][f\"question_{question}\"] = similarity\n",
        "\n",
        "    return similarities, similarity_dict\n",
        "\n",
        "# Extract information\n",
        "extracted_data = extract_all_information(bases, peers)\n",
        "\n",
        "# Compute transformer-based similarities\n",
        "similarities, similarity_dict = calculate_transformer_similarity(extracted_data, bases)\n",
        "\n",
        "# Print results\n",
        "for sim in similarities:\n",
        "    print(f\"Peer: {sim['peer']}, Base: {sim['base']}, Question: {sim['question']}, Similarity: {sim['similarity']:.4f}\")\n",
        "\n",
        "# Save the results as CSV\n",
        "similarity_df = pd.DataFrame(similarities)\n",
        "similarity_df.to_csv('similarities2.csv', index=False)\n",
        "\n",
        "# Save the results as a dictionary\n",
        "import json\n",
        "with open('similarity_dict.json', 'w') as f:\n",
        "    json.dump(similarity_dict, f)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJ22tUutkoUw",
        "outputId": "25ddbe9b-e690-4ec8-9f51-fae31509c4a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Peer: Healthily, Base: zava, Question: 1, Similarity: 0.4493\n",
            "Peer: Healthily, Base: zava, Question: 2, Similarity: 0.3115\n",
            "Peer: Tele clinic, Base: zava, Question: 1, Similarity: 0.4335\n",
            "Peer: Tele clinic, Base: zava, Question: 2, Similarity: 0.4020\n",
            "Peer: Practo, Base: zava, Question: 1, Similarity: 0.6025\n",
            "Peer: Practo, Base: zava, Question: 2, Similarity: 0.4912\n",
            "Peer: Urban, Base: zava, Question: 1, Similarity: 0.3421\n",
            "Peer: Urban, Base: zava, Question: 2, Similarity: 0.2763\n",
            "Peer: Intendu, Base: zava, Question: 1, Similarity: 0.1656\n",
            "Peer: Intendu, Base: zava, Question: 2, Similarity: 0.6781\n",
            "Peer: Watch your health, Base: zava, Question: 1, Similarity: 0.7765\n",
            "Peer: Watch your health, Base: zava, Question: 2, Similarity: 0.4310\n",
            "Peer: Cura, Base: zava, Question: 1, Similarity: 0.6641\n",
            "Peer: Cura, Base: zava, Question: 2, Similarity: 0.4688\n",
            "Peer: Health hero, Base: zava, Question: 1, Similarity: 0.3800\n",
            "Peer: Health hero, Base: zava, Question: 2, Similarity: 0.7702\n",
            "Peer: Biohm, Base: zava, Question: 1, Similarity: 0.3347\n",
            "Peer: Biohm, Base: zava, Question: 2, Similarity: 0.1594\n",
            "Peer: Qured, Base: zava, Question: 1, Similarity: 0.5368\n",
            "Peer: Qured, Base: zava, Question: 2, Similarity: 0.4003\n",
            "Peer: Klikdoctor, Base: zava, Question: 1, Similarity: 0.3758\n",
            "Peer: Klikdoctor, Base: zava, Question: 2, Similarity: 0.3911\n",
            "Peer: Body collective, Base: zava, Question: 1, Similarity: 0.4021\n",
            "Peer: Body collective, Base: zava, Question: 2, Similarity: 0.3133\n",
            "Peer: Penda health, Base: zava, Question: 1, Similarity: 0.4898\n",
            "Peer: Penda health, Base: zava, Question: 2, Similarity: 0.3704\n",
            "Peer: Pathkind labs, Base: zava, Question: 1, Similarity: 0.3663\n",
            "Peer: Pathkind labs, Base: zava, Question: 2, Similarity: 0.3645\n",
            "Peer: Lora, Base: zava, Question: 1, Similarity: 0.5721\n",
            "Peer: Lora, Base: zava, Question: 2, Similarity: 0.5586\n",
            "Peer: Gene box, Base: zava, Question: 1, Similarity: 0.2995\n",
            "Peer: Gene box, Base: zava, Question: 2, Similarity: 0.3229\n",
            "Peer: Bosta, Base: bozo, Question: 1, Similarity: 0.8770\n",
            "Peer: Bosta, Base: bozo, Question: 2, Similarity: 0.5413\n",
            "Peer: Zoomo, Base: bozo, Question: 1, Similarity: 0.3396\n",
            "Peer: Zoomo, Base: bozo, Question: 2, Similarity: 0.3426\n",
            "Peer: Grovy, Base: bozo, Question: 1, Similarity: 0.4835\n",
            "Peer: Grovy, Base: bozo, Question: 2, Similarity: 0.5323\n",
            "Peer: Vahak, Base: bozo, Question: 1, Similarity: 0.5991\n",
            "Peer: Vahak, Base: bozo, Question: 2, Similarity: 0.3509\n",
            "Peer: Drive yello, Base: bozo, Question: 1, Similarity: 0.8006\n",
            "Peer: Drive yello, Base: bozo, Question: 2, Similarity: 0.5883\n",
            "Peer: Paps, Base: bozo, Question: 1, Similarity: 0.9053\n",
            "Peer: Paps, Base: bozo, Question: 2, Similarity: 0.3689\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BndxCWu8tjp1",
        "outputId": "a570bcfe-4571-4428-9b31-a32db1c5aac8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentence-transformers\n",
            "  Downloading sentence_transformers-3.0.1-py3-none-any.whl (227 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.1/227.1 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: transformers<5.0.0,>=4.34.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.42.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.4)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.3.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.25.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.11.4)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.23.5)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2023.6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (2.3.1)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m63.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (2024.5.15)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.4.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.19.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2024.7.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, sentence-transformers\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105 sentence-transformers-3.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Qe8Q3DyBU1d1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "id": "CzaTJVO_uLCy",
        "outputId": "15722572-bd6f-47ec-c7e7-75699b4f90d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-d5df0069828e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    131\u001b[0m   )\n\u001b[1;32m    132\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Actual iteration"
      ],
      "metadata": {
        "id": "8UoLHi5ksC0Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline, AutoModelForQuestionAnswering, AutoTokenizer\n",
        "import json\n",
        "import csv\n",
        "import os\n",
        "\n",
        "# Establish dictionaries\n",
        "questions_dict = {\n",
        "    'question1': \"What does {} provide?\",\n",
        "    'question2': \"What is {} vertical focus?\",\n",
        "    'question3': \"Who are {} consumers?\"\n",
        "}\n",
        "\n",
        "# Define the company descriptions\n",
        "descriptions = {\n",
        "    \"instabase\": \"Instabase provides a platform for automating complex business processes and document workflows using machine learning and artificial intelligence. Targeting enterprises across various industries, their focus is on streamlining operations and improving efficiency through advanced automation and data processing solutions.\",\n",
        "    \"frame\": \"Frame offers a cloud-based platform that enables users to run desktop applications in a web browser, targeting businesses and organizations needing flexible and scalable virtual desktop solutions. Their focus is on cloud computing and virtual desktop infrastructure (VDI), providing a seamless user experience for accessing applications remotely.\",\n",
        "    \"rtbrick\": \"rtBrick delivers network automation solutions designed for service providers and telecom operators, targeting companies looking to modernize their network infrastructure. Their focus is on enabling programmable, cloud-native networks through software-defined networking (SDN) and network function virtualization (NFV).\",\n",
        "    \"meshare\": \"meShare provides a cloud-based video surveillance and security solution, targeting homeowners and businesses needing reliable and scalable security systems. Their focus is on offering a comprehensive platform for managing and accessing video surveillance footage from multiple cameras and locations.\",\n",
        "    \"backbox\": \"Backbox offers automated network security and backup solutions, targeting businesses needing robust security and disaster recovery systems for their network infrastructure. Their focus is on simplifying network management and ensuring data protection through automated backup and recovery processes.\",\n",
        "    \"origin_protocol\": \"Origin Protocol develops decentralized applications and blockchain-based solutions to enhance digital transactions and data security. Targeting developers and enterprises, their focus is on enabling secure, transparent, and efficient transactions through blockchain technology and smart contracts.\",\n",
        "    \"petuum\": \"Petuum provides a platform for machine learning and artificial intelligence that simplifies the development and deployment of AI models. Targeting enterprises and researchers, their focus is on making advanced AI accessible and scalable, facilitating the integration of machine learning into various applications and systems.\"\n",
        "}\n",
        "\n",
        "# Create the new dictionary format\n",
        "formatted_descriptions = {\n",
        "    'instabase': [\n",
        "        descriptions['instabase'],\n",
        "        \"base\",\n",
        "        \"instabase\"\n",
        "    ],\n",
        "    'frame': [\n",
        "        descriptions['frame'],\n",
        "        \"peer\",\n",
        "        \"frame\"\n",
        "    ],\n",
        "    'rtbrick': [\n",
        "        descriptions['rtbrick'],\n",
        "        \"peer\",\n",
        "        \"rtbrick\"\n",
        "    ],\n",
        "    'meshare': [\n",
        "        descriptions['meshare'],\n",
        "        \"peer\",\n",
        "        \"meshare\"\n",
        "    ],\n",
        "    'backbox': [\n",
        "        descriptions['backbox'],\n",
        "        \"peer\",\n",
        "        \"backbox\"\n",
        "    ],\n",
        "    'origin_protocol': [\n",
        "        descriptions['origin_protocol'],\n",
        "        \"peer\",\n",
        "        \"origin_protocol\"\n",
        "    ],\n",
        "    'petuum': [\n",
        "        descriptions['petuum'],\n",
        "        \"peer\",\n",
        "        \"petuum\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Initialize the info_extracted dictionary with empty lists for each company\n",
        "info_extracted = {company: [] for company in [\n",
        "    'instabase',\n",
        "    'frame',\n",
        "    'rtbrick',\n",
        "    'meshare',\n",
        "    'backbox',\n",
        "    'origin_protocol',\n",
        "    'petuum'\n",
        "]}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Define the function for information extraction\n",
        "def information_extraction(questions, descriptions, info_extracted, nlp):\n",
        "    for description_key, description_list in descriptions.items():\n",
        "        company_name = description_list[2]\n",
        "        context = description_list[0]\n",
        "        for question_key, question_template in questions.items():\n",
        "            # Format the question with the company name\n",
        "            question_text = question_template.format(company_name)\n",
        "            QA_input = {\n",
        "                'question': question_text,\n",
        "                'context': context\n",
        "            }\n",
        "            # Get predictions\n",
        "            res = nlp(QA_input)\n",
        "            response = res['answer']\n",
        "            # Store predictions\n",
        "            info_extracted[company_name].append(response)\n",
        "\n",
        "# Load model & tokenizer\n",
        "model_name = \"deepset/roberta-base-squad2\"\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "nlp = pipeline('question-answering', model=model, tokenizer=tokenizer)\n",
        "\n",
        "# Execute information extraction\n",
        "information_extraction(questions_dict, formatted_descriptions, info_extracted, nlp)\n",
        "\n",
        "# Print the results\n",
        "for company, answers in info_extracted.items():\n",
        "    print(f\"{company}: {answers}\")\n",
        "\n",
        "import json\n",
        "\n",
        "\n",
        "with open('/content/drive/My Drive/peers_synthetic/dictionary (v2)/instabase/info_extracted_instabase.json', 'w') as f:\n",
        "    json.dump(info_extracted, f)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wfebRsotfjNt",
        "outputId": "f77416a6-033c-4a12-fbc2-149a82f97e65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "instabase: ['a platform for automating complex business processes and document workflows', 'streamlining operations and improving efficiency', 'enterprises']\n",
            "frame: ['a cloud-based platform', 'cloud computing and virtual desktop infrastructure', 'businesses and organizations']\n",
            "rtbrick: ['network automation solutions', 'enabling programmable, cloud-native networks', 'service providers and telecom operators']\n",
            "meshare: ['cloud-based video surveillance and security solution', 'managing and accessing video surveillance footage from multiple cameras and locations', 'homeowners and businesses']\n",
            "backbox: ['automated network security and backup solutions', 'simplifying network management and ensuring data protection', 'businesses']\n",
            "origin_protocol: ['decentralized applications and blockchain-based solutions to enhance digital transactions and data security', 'enabling secure, transparent, and efficient transactions through blockchain technology and smart contracts', 'developers and enterprises']\n",
            "petuum: ['a platform for machine learning and artificial intelligence', 'making advanced AI accessible and scalable', 'enterprises and researchers']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "\n",
        "with open('/content/drive/My Drive/peers_synthetic/dictionary (v2)/instabase/description_dict_instabase.json', 'w') as f:\n",
        "    json.dump(formatted_descriptions, f)"
      ],
      "metadata": {
        "id": "63h835ntSC3Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Install necessary libraries\n",
        "#!pip install sentence-transformers scikit-learn\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "# Initialize the sBERT model\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "\n",
        "\n",
        "# Get all the keys in the dictionary\n",
        "keys = list(info_extracted.keys())\n",
        "\n",
        "# Function to compute cosine similarities\n",
        "def compute_similarities(info_extracted):\n",
        "    similarities = {}\n",
        "\n",
        "    # Iterate through each pair of keys\n",
        "    for i in range(1, len(keys)):\n",
        "        current_key = keys[i]\n",
        "        similarities[current_key] = []\n",
        "\n",
        "        for j in range(3):\n",
        "            # Embed sentences\n",
        "            embeddings1 = model.encode([info_extracted['instabase'][j]])\n",
        "            embeddings2 = model.encode([info_extracted[current_key][j]])\n",
        "\n",
        "            # Calculate cosine similarity\n",
        "            similarity = cosine_similarity(embeddings1, embeddings2)[0][0]\n",
        "            similarities[current_key].append(similarity)\n",
        "\n",
        "    return similarities\n",
        "\n",
        "# Calculate similarities\n",
        "similarity_results = compute_similarities(info_extracted)\n",
        "\n",
        "# Display the results\n",
        "for key, values in similarity_results.items():\n",
        "    print(f\"Similarities with 'zava' and '{key}': {values}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1vX57XEugdkL",
        "outputId": "a9ee422c-7389-408b-b334-95357ca5c2d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similarities with 'zava' and 'frame': [0.3597218, 0.21445343, 0.7183789]\n",
            "Similarities with 'zava' and 'rtbrick': [0.4145234, 0.21748137, 0.43484122]\n",
            "Similarities with 'zava' and 'meshare': [0.10694204, 0.16815665, 0.46712226]\n",
            "Similarities with 'zava' and 'backbox': [0.24286373, 0.28274375, 0.74353886]\n",
            "Similarities with 'zava' and 'origin_protocol': [0.20307647, 0.27726978, 0.641783]\n",
            "Similarities with 'zava' and 'petuum': [0.34303653, 0.30774298, 0.7322169]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "similarity_results_native = {key: [float(value) for value in values] for key, values in similarity_results.items()}\n",
        "\n",
        "with open('/content/drive/My Drive/peers_synthetic/dictionary (v2)/instabase/similarity_results_instabase.json', 'w') as f:\n",
        "    json.dump(similarity_results_native, f)"
      ],
      "metadata": {
        "id": "c5BSu4W1Tdsi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "\n",
        "with open('/content/drive/My Drive/peers_synthetic/dictionary (v2)/borzo/info_extracted_borzo.json', 'w') as f:\n",
        "    json.dump(info_extracted, f)"
      ],
      "metadata": {
        "id": "KgebPLv4Uj41"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Provided dictionary\n",
        "data = {\n",
        "    \"frame\": [0.3597218096256256, 0.21445342898368835, 0.7183789014816284], \"rtbrick\": [0.4145233929157257, 0.21748137474060059, 0.43484121561050415], \"meshare\": [0.10694204270839691, 0.16815665364265442, 0.46712225675582886], \"backbox\": [0.242863729596138, 0.28274375200271606, 0.7435388565063477], \"origin_protocol\": [0.20307646691799164, 0.27726978063583374, 0.6417829990386963], \"petuum\": [0.3430365324020386, 0.30774298310279846, 0.7322168946266174]\n",
        "}\n",
        "\n",
        "# Convert keys to lowercase and prepare data for CSV\n",
        "csv_data = [[key.lower()] + value for key, value in data.items()]\n",
        "\n",
        "# Define CSV file name and path\n",
        "csv_file = \"/content/drive/MyDrive/peers_synthetic/instabase_scores.csv\"\n",
        "\n",
        "\n",
        "# Create the output directory if it doesn't exist\n",
        "output_dir = os.path.dirname(csv_file)\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "# Write data to CSV\n",
        "with open(csv_file, 'w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    # Write header\n",
        "    writer.writerow(['company', 'score1', 'score2', 'score3'])\n",
        "    # Write data rows\n",
        "    writer.writerows(csv_data)\n",
        "\n",
        "print(f\"Data successfully written to {csv_file}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vEq1Zd7HNBKQ",
        "outputId": "991ff72c-2336-48e5-98e0-4dcd69382460"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Data successfully written to /content/drive/MyDrive/peers_synthetic/instabase_scores.csv\n"
          ]
        }
      ]
    }
  ]
}